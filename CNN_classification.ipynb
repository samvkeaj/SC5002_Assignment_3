{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device: use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define transforms for train, validation, and test\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),  # rotate by Â±10 degrees\n",
    "    transforms.RandomResizedCrop(250, scale=(0.9, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((250, 250)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Arborio']\n",
      "Total images: 1104\n",
      "Training images: 883\n",
      "Validation images: 177\n",
      "Test images: 44\n"
     ]
    }
   ],
   "source": [
    "# Create a custom Dataset wrapper to apply transforms to a subset\n",
    "class TransformSubset(Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "dataset_path = 'Rice_Image_Dataset'\n",
    "if not os.path.isdir(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset folder '{dataset_path}' not found.\")\n",
    "\n",
    "# Load the full dataset without applying any transform yet\n",
    "full_dataset = ImageFolder(root=dataset_path, transform=None)\n",
    "print(\"Classes:\", full_dataset.classes)\n",
    "\n",
    "# Shuffle and split: 80% train, and the remaining 20% will be split into 80% validation and 20% test.\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "remainder = total_size - train_size\n",
    "test_size = int(0.2 * remainder)  # 20% of the remaining 20%\n",
    "val_size = remainder - test_size   # rest for validation\n",
    "\n",
    "# Random split the dataset\n",
    "train_subset, remainder_subset = random_split(full_dataset, [train_size, remainder])\n",
    "val_subset, test_subset = random_split(remainder_subset, [val_size, test_size])\n",
    "\n",
    "print(f\"Total images: {total_size}\")\n",
    "print(f\"Training images: {len(train_subset)}\")\n",
    "print(f\"Validation images: {len(val_subset)}\")\n",
    "print(f\"Test images: {len(test_subset)}\")\n",
    "\n",
    "# Wrap the subsets with the proper transforms\n",
    "train_dataset = TransformSubset(train_subset, train_transform)\n",
    "val_dataset = TransformSubset(val_subset, val_test_transform)\n",
    "test_dataset = TransformSubset(test_subset, val_test_transform)\n",
    "\n",
    "# Create DataLoaders with reduced batch size and num_workers set to 0 for debugging.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class RiceCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RiceCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 250x250 -> 125x125\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 125x125 -> ~62x62\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # ~62x62 -> ~31x31\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)   # ~31x31 -> ~15x15\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),  # Increased dropout for regularization\n",
    "            nn.Linear(128, 5)  # 5 classes for rice types\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and move it to the device\n",
    "model = RiceCNN().to(device)\n",
    "print(model)\n",
    "\n",
    "# Define loss function and optimizer with weight decay for regularization.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Early stopping parameters\n",
    "num_epochs = 30  # Maximum epochs (training may stop earlier)\n",
    "patience = 3   # Number of epochs to wait for improvement before stopping\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Training and validation loop with detailed logging and timing per epoch\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Log progress every 50 batches\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}] processed...\")\n",
    "    \n",
    "    train_loss = running_loss / total\n",
    "    train_acc = 100.0 * correct / total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "    val_loss /= val_total\n",
    "    val_acc = 100.0 * val_correct / val_total\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f} seconds.\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Early stopping check based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        # Save the best model so far\n",
    "        torch.save(model.state_dict(), 'best_rice_cnn.pth')\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        print(f\"Validation loss did not improve. Early stopping counter: {early_stopping_counter}/{patience}\")\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "# Optionally, save the final model if needed\n",
    "torch.save(model.state_dict(), 'rice_cnn_final.pth')\n",
    "print(\"Final model saved as rice_cnn_final.pth\")\n",
    "\n",
    "# ------------------------------\n",
    "# Testing phase: Evaluate the best model on the test set\n",
    "# ------------------------------\n",
    "# Load the best model (saved based on validation loss)\n",
    "best_model = RiceCNN().to(device)\n",
    "best_model.load_state_dict(torch.load('best_rice_cnn.pth', map_location=device))\n",
    "best_model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = best_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_loss /= test_total\n",
    "test_acc = 100.0 * test_correct / test_total\n",
    "print(f\"Best Model Test Loss: {test_loss:.4f}, Best Model Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RiceCNN().to(device)\n",
    "model.load_state_dict(torch.load('best_rice_cnn.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Gather predictions and true labels from the test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute classification report: precision, recall, F1 score for each class.\n",
    "target_names = full_dataset.classes  # assuming full_dataset is defined and contains the classes\n",
    "print(classification_report(all_labels, all_preds, target_names=target_names))\n",
    "\n",
    "# Compute and plot the confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', \n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
