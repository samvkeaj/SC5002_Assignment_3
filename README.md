# SC5002_Assignment_3

Dataset: The dataset originates from a **_** setting. In this dataset, there is **_** data for many variables that have shown to be related to **_**. 

There are **_** features in the dataset which provide valuable information related to **_**:

Feature 1: Description
Feature 2: Description
 diabetes risk.

Additionally, mimicking the complexities of data collection from **_** in real life, there exist certain measurements with no available data. This also presents an opportunity to utilise data cleaning and data imputation methods.

Project: Here, we attempt to use **classification and clustering** to see if accurate patterns can be identified by **_**, . If the prediction proves to be reliable, then it means that there exists a feasible relationship between **_**. 

**Steps taken:**

Briefly, the steps taken are as follows:

Vetting of a few different datasets - some datasets were too huge, or had questionable integrity
Superficial check for missing values, by looking for empty strings or null values
Deeper check of dataset for data integrity issues
Data cleaning - removing certain rows, and imputing missing data for others
Creating function for model training, with built-in encoding of categorical values and scaling of numerical values
Specifying input features and continous output (for **clustering only?**)
**Perform train-test split, and calculate performance metrics (MSE and R2)
Conduct k-fold cross validation, and calculate performance metrics**
Analysis and discussion of results
Insights: **_**

**Link to slides:** a

**Individual Contributions:** (Clearly highlight the contributions of each team member in both the GitHub repository and the video)

Both **_** and **_** were involved in selecting datasets, with a total of 2 datasets examined by **_** and 1 dataset examined by Yuen In before the final dataset was selected for use.
**_** and **_** added text cells with explanations, such that the notebook had a smooth logical flow.
**_** was responsible for handling missing values in the dataset.
**_** was responsible for encoding categorical variables using One-Hot Encoding, and scaling numerical features using Min-Max Scaling.
**_** was responsible for splitting the data into test and training data sets.
**_** wrote the bulk of the code for Linear Regression, Ridge Regression, and k-fold cross validation, with adaptation of the code to the datasets done by Bryant.
**_** was responsible for comparing the models using Mean Squared Error (MSE) and RÂ² score, and analysing cases where Linear Regression overfits and where Ridge Regression helps prevent overfitting.
**_** was responsible for interpreting the effect of different alpha values on the MSE and R^2 scores of the Ridge Regression Model, as well as discussing practical applications where each model would be more suitable.
**_** was responsible for the overall project management, including the GitHub repository.
**_** and **_** put together the slides.
